{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized Replay Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(100000) # replay buffer size\n",
    "BATCH_SIZE = 64           # minibatch size\n",
    "GAMMA = 0.99              # discount rate\n",
    "TAU = .001                # weight for soft update of target q-network parameters\n",
    "LR = .0005                # learning rate for backprop\n",
    "UPDATE_EVERY = 4          # number of time steps between backward passes on local q-network\n",
    "eps=0.99                  # starting value for epsilon, the probability of randomly selecting an action  \n",
    "decay=0.015               # the rate of decay in epsilon\n",
    "min_eps=0.01              # minimum value of epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below.  Please run the next code cell without making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# please do not modify the line below\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Value function template\"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y = self.fc3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object\"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from the memory buffer\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        # reshape and return experience components to device (CPU or GPU, set elsewhere)\n",
    "        # from_numpy() creates pytorch tensor from numpy array\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def buffer_to_list(self):\n",
    "        return list(self.memory)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of replay buffer memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, seed, eps=0.99, decay=0.015, min_eps=0.01):\n",
    "        \"\"\"Initialize the learning agent.\n",
    "        \n",
    "        params:\n",
    "            decay: (0, 1] epsilon is multplied by (1 - decay) after each episode. \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.eps = eps\n",
    "        self.decay = decay\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        #Initialize local and target q-networks\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        # Initialize optimizer to run gradient descent on local q-network\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR) # LR is environment variable\n",
    "        \n",
    "        # Get experiences for training from replay buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step = 0 # Time step counter\n",
    "        self.replay_buffer = None\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save step to replay buffer\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        # Run learning cycle (backprop) every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # Make sure there are enough samples in buffer to fill batch\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "        # apply epsilon decay after each episode\n",
    "        if done:\n",
    "            self.eps = max(self.min_eps, self.eps * (1 - self.decay))\n",
    "            # Save replay buffer to list\n",
    "            self.replay_buffer = self.memory.buffer_to_list()\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Get policy action given current state\"\"\"\n",
    "        # unsqueeze() returns new tensor with extra dimension inserted at position\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        # put qnetwork_local in eval mode (disable gradient computations)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            # get inference\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        # put qnetwork_local back in train mode\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # select epsilon-greedy action from inference values\n",
    "        if random.random() > self.eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update parameters of local qnetwork using backward pass\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values for next states from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Get expected Q values from local network\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0-tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent):\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name] \n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    state_size = len(state)\n",
    "    score = 0  \n",
    "\n",
    "    while True:\n",
    "        # get next experience\n",
    "        action = agent.act(state)                      # get policy-based action\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "        # add experience to replay buffer & learn every UPDATE_EVERY steps\n",
    "        agent.step(state, action, reward, next_state, done) \n",
    "\n",
    "        # increment score\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            return score, agent.replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_banana_agent(agent, n_episodes=500, max_t=2000):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        score, _ = run_episode(agent)\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.48\n",
      "Episode 200\tAverage Score: 6.94\n",
      "Episode 300\tAverage Score: 11.05\n",
      "Episode 377\tAverage Score: 13.01\n",
      "Environment solved in 277 episodes!\tAverage Score: 13.01\n",
      "\n",
      "training time: 300.1870413050001s\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# update environment variables if desired\n",
    "\n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.01)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=500, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Rewards per Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f271674ed68>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the score trend over episodes\n",
    "plt.plot(scores)\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"Training scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* The banana game trained fastest when exploration (epsilon) decayed quickly to a minimal level (1% exploration, 99% greedy action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "w = agent.qnetwork_local.parameters()\n",
    "\n",
    "weights = []\n",
    "for elem in w:\n",
    "    weights.append(elem.detach().numpy())\n",
    "\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Prioritized Replay\n",
    "\n",
    "Picking up a banana -- of either color -- is a relatively infrequent occurence in the banana game. Would prioritizing replay of pickup experiences improve learning?\n",
    "\n",
    "First, let's measure how frequently an agent receives a non-zero reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_banana_agent(agent, n_episodes=350, max_t=2000):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        score, replay_buffer = run_episode(agent)\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores, replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.01)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores1, replay_buffer_1 = train_banana_agent(agent, n_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of positive, negative and zero-valued rewards during play confirm that non-zero rewards are very infrequent events (<1%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 (0.2%) negatives out of 30000 experiences\n",
      "144 (0.5%) positives out of 30000 experiences\n",
      "29799 (99.3%) zeros out of 30000 experiences\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "buffers = [replay_buffer_1]\n",
    "for buffer in buffers:\n",
    "    r = [e[2] for e in buffer]\n",
    "    rewards = rewards + r\n",
    "negatives = rewards.count(-1)\n",
    "positives = rewards.count(1)\n",
    "zeros = rewards.count(0)\n",
    "total = len(rewards)\n",
    "print('{} ({:.1%}) negatives out of {} experiences'.format(negatives, negatives/total, total))\n",
    "print('{} ({:.1%}) positives out of {} experiences'.format(positives, positives/total, total))\n",
    "print('{} ({:.1%}) zeros out of {} experiences'.format(zeros, zeros/total, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prioritize Experiences with Positive and Negative Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, p):\n",
    "        \"\"\"Initialize a ReplayBuffer object. Enable prioritized experience replay\n",
    "        by creating separate buffers for neutral experiences (r==0) vs prioritized\n",
    "        experiences (positive or negative reward).\"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.prioritized_memory = deque(maxlen=buffer_size)\n",
    "        self.neutral_memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.p = p #p=proportion of experiences to sample from prioritized memory\n",
    "        self.check_batch_size_vs_proportion()\n",
    "        \n",
    "    class Error(Exception):\n",
    "        pass\n",
    "    \n",
    "    class BatchProportionError(Error):\n",
    "        pass        \n",
    "        \n",
    "    def check_batch_size_vs_proportion(self):\n",
    "        try:\n",
    "            if (self.p * self.batch_size) - int(self.p * self.batch_size) == 0:\n",
    "                return\n",
    "            else:\n",
    "                raise BatchProportionError\n",
    "        except:\n",
    "            print('Invalid ReplayBuffer: Proportion of batch for prioritized replay must be a whole number (p * batch_size is integer).')\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the appropriate buffer\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        if self.p>0 and reward in [-1,1]:\n",
    "            self.prioritized_memory.append(e) \n",
    "        else:\n",
    "            self.neutral_memory.append(e)\n",
    "            \n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from the memory buffer\"\"\"\n",
    "        if self.p>0:\n",
    "            experiences = (\n",
    "                random.sample(self.prioritized_memory, k=int(self.batch_size*self.p)) + \n",
    "                random.sample(self.neutral_memory, k=int(self.batch_size*(1-self.p)))           \n",
    "            )\n",
    "        else:\n",
    "            experiences = random.sample(self.neutral_memory, k=self.batch_size)  \n",
    "        random.shuffle(experiences)\n",
    "        # reshape and return experience components to device (CPU or GPU, set elsewhere)\n",
    "        # from_numpy() creates pytorch tensor from numpy array\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "#     def buffer_to_list(self, buffer):\n",
    "#         return list(buffer)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of replay buffer memory\"\"\"\n",
    "        return len(self.neutral_memory) + len(self.prioritized_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Agent Class to Implement Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, seed, eps=0.99, decay=0.005, min_eps=0.05):\n",
    "        \"\"\"Initialize the learning agent.\n",
    "        \n",
    "        params:\n",
    "            decay: (0, 1] epsilon is multplied by (1 - decay) after each episode. \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.eps = eps\n",
    "        self.decay = decay\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        #Initialize local and target q-networks\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        # Initialize optimizer to run gradient descent on local q-network\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR) # LR is environment variable\n",
    "        \n",
    "        # Get experiences for training from replay buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed, P)\n",
    "        self.t_step = 0 # Time step counter\n",
    "        self.replay_buffer = None\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save step to replay buffer\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Run learning cycle (backprop) every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # Make sure there are enough samples in buffer to fill batch\n",
    "            if (len(self.memory.prioritized_memory) >= P * BATCH_SIZE and \n",
    "                len(self.memory.neutral_memory) >= (1-P) * BATCH_SIZE):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "        # apply epsilon decay after each episode\n",
    "        if done:\n",
    "            self.eps = max(self.min_eps, self.eps * (1 - self.decay))\n",
    "            # Save replay buffer to list\n",
    "#             self.replay_buffer = self.memory.buffer_to_list()\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Get policy action given current state\"\"\"\n",
    "        # unsqueeze() returns new tensor with extra dimension inserted at position\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        # put qnetwork_local in eval mode (disable gradient computations)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            # get inference\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        # put qnetwork_local back in train mode\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # select epsilon-greedy action from inference values\n",
    "        if random.random() > self.eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update parameters of local qnetwork using backward pass\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values for next states from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Get expected Q values from local network\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0-tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized replay proportion P = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.06\n",
      "Episode 200\tAverage Score: 1.05\n",
      "Episode 300\tAverage Score: 2.56\n",
      "Episode 350\tAverage Score: 3.05training time: 273.23979966800016s\n"
     ]
    }
   ],
   "source": [
    "P = 0.125\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('training time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized replay proportion P = 0.0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.10\n",
      "Episode 200\tAverage Score: 0.342\n",
      "Episode 300\tAverage Score: 0.68\n",
      "Episode 350\tAverage Score: 1.19training time: 276.3147756540002s\n"
     ]
    }
   ],
   "source": [
    "P = 0.0625\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.05)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.05)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update less frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_EVERY = 12\n",
    "P = 0\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.05)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prioritize positive rewards only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, p):\n",
    "        \"\"\"Initialize a ReplayBuffer object. Enable prioritized experience replay\n",
    "        by creating separate buffers for neutral experiences (r==0) vs prioritized\n",
    "        experiences (positive or negative reward).\"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.prioritized_memory = deque(maxlen=buffer_size)\n",
    "        self.neutral_memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.p = p #p=proportion of experiences to sample from prioritized memory\n",
    "        self.check_batch_size_vs_proportion()\n",
    "        \n",
    "    class Error(Exception):\n",
    "        pass\n",
    "    \n",
    "    class BatchProportionError(Error):\n",
    "        pass        \n",
    "        \n",
    "    def check_batch_size_vs_proportion(self):\n",
    "        try:\n",
    "            if (self.p * self.batch_size) - int(self.p * self.batch_size) == 0:\n",
    "                return\n",
    "            else:\n",
    "                raise BatchProportionError\n",
    "        except:\n",
    "            print('Invalid ReplayBuffer: Proportion of batch for prioritized replay must be a whole number (p * batch_size is integer).')\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the appropriate buffer\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        if self.p>0 and reward>0:\n",
    "            self.prioritized_memory.append(e)\n",
    "        else:\n",
    "            self.neutral_memory.append(e)\n",
    "            \n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from the memory buffer\"\"\"\n",
    "        if self.p>0:\n",
    "            experiences = (\n",
    "                random.sample(self.prioritized_memory, k=int(self.batch_size*self.p)) + \n",
    "                random.sample(self.neutral_memory, k=int(self.batch_size*(1-self.p)))           \n",
    "            )\n",
    "        else:\n",
    "            random.sample(self.neutral_memory, k=1)  \n",
    "        random.shuffle(experiences)\n",
    "        # reshape and return experience components to device (CPU or GPU, set elsewhere)\n",
    "        # from_numpy() creates pytorch tensor from numpy array\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "#     def buffer_to_list(self, buffer):\n",
    "#         return list(buffer)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of replay buffer memory\"\"\"\n",
    "        return len(self.neutral_memory) + len(self.prioritized_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.125\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.05)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.0625\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, \n",
    "              eps=0.99, decay=0.015, min_eps=0.05)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.0\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "score = 0  \n",
    "\n",
    "# create agent; initialize local and target q-network functions\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=561, eps=0.99, decay=0.015)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "scores = train_banana_agent(agent=agent, n_episodes=350, max_t=2000)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\ntraining time: {}s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
